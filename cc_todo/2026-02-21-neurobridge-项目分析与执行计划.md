# NeuroBridge é¡¹ç›®åˆ†æä¸æ‰§è¡Œè®¡åˆ’

**åˆ†ææ—¥æœŸ**ï¼š2026-02-21
**åˆ†æè€…**ï¼šClaude Code
**é¡¹ç›®ä»“åº“**ï¼š`/root/autodl-tmp/NeuroBridge`

---

## 1. é¡¹ç›®æ¦‚è¿°

NeuroBridge æ—¨åœ¨æ„å»ºä¸€ä¸ªä» spiking/MUA ç¥ç»æ•°æ®åˆ°å›¾åƒé‡å»ºçš„å®Œæ•´ pipelineï¼š
1. **è‡ªç›‘ç£é¢„è®­ç»ƒ**ï¼šåœ¨ POYO çš„ spike-level tokenization + PerceiverIO æ¶æ„ä¸Šå®ç° MtM é£æ ¼çš„å¤šä»»åŠ¡ masking é¢„è®­ç»ƒ
2. **è§†è§‰å¯¹é½**ï¼šå°†é¢„è®­ç»ƒç¥ç»è¡¨å¾é€šè¿‡å¯¹æ¯”å­¦ä¹ æ˜ å°„åˆ° CLIP è§†è§‰è¯­ä¹‰ç©ºé—´
3. **å›¾åƒé‡å»º**ï¼šé€šè¿‡ Diffusion Adapter æ³¨å…¥ Stable Diffusion ç”Ÿæˆé‡å»ºå›¾åƒ

**ç›®æ ‡å‘è¡¨**ï¼šNeurIPS / ICML / Nature Methods

---

## 2. Proposal åˆç†æ€§å®¡æŸ¥

### 2.1 é—®é¢˜æ¸…å•

| # | ä¸¥é‡ç¨‹åº¦ | é—®é¢˜æè¿° | è§£å†³æ–¹æ¡ˆ | çŠ¶æ€ |
|---|---------|---------|---------|------|
| 1 | ğŸ”´ ä¸¥é‡ | TVSD æä¾› MUAï¼ˆè¿ç»­ä¿¡å·ï¼‰ï¼Œä¸æ˜¯ spike-sorted æ•°æ®ï¼Œä¸ POYO çš„ spike-level tokenization ä¸å…¼å®¹ | é‡‡ç”¨ CaPOYO æ¨¡å¼ï¼š`input_value_map = nn.Linear(1, dim//2)` æ˜ å°„è¿ç»­å€¼ + unit_emb æ‹¼æ¥ | æ–¹æ¡ˆå·²ç¡®å®š |
| 2 | ğŸ”´ ä¸¥é‡ | Proposal æå‡º d_model=512, depth=12ï¼ˆ~100M paramsï¼‰ï¼Œå½“å‰ GPU ä¸º RTX 4090 24GBï¼Œæ— æ³•è®­ç»ƒ | ä» dim=128, depth=6 (~5M params) èµ·æ­¥ï¼ŒéªŒè¯åé€æ­¥æ‰©å¤§ | æ–¹æ¡ˆå·²ç¡®å®š |
| 3 | ğŸŸ¡ ä¸­ç­‰ | Poisson NLL å‡è®¾ç¦»æ•£è®¡æ•°ï¼Œä¸é€‚ç”¨äº MUA è¿ç»­ä¿¡å· | å¯¹ spike count ç”¨ Poisson NLLï¼Œå¯¹ MUA ç”¨ MSE | æ–¹æ¡ˆå·²ç¡®å®š |
| 4 | ğŸŸ¡ ä¸­ç­‰ | Allen ä»… 118 å¼ å›¾ä¸è¶³ä»¥åšå›¾åƒé‡å»ºä¸»åŠ› | TVSD æ˜¯æ ¸å¿ƒï¼ˆ22K å¼ ï¼‰ï¼ŒAllen ä»…ä½œè¾…åŠ©éªŒè¯ | proposal å·²è¯†åˆ« |
| 5 | ğŸŸ¢ ä½ | IBL é¢„è®­ç»ƒå¯¹è§†è§‰é‡å»ºçš„è¿ç§»ä»·å€¼æœªéªŒè¯ | åœ¨å•å¡èµ„æºä¸‹æš‚ä¸æ•´åˆ IBLï¼Œä¸“æ³¨ Allen + TVSD | æ–¹æ¡ˆå·²ç¡®å®š |
| 6 | ğŸŸ¢ ä½ | é¡¹ç›®èŒƒå›´è¿‡å¤§ï¼ˆ4 åˆ›æ–°ç‚¹ + 7 å®éªŒ + 3 æ•°æ®æºï¼‰ | å‰Šå‡åˆ°æœ€å°å¯å‘è¡¨å•å…ƒ | æ–¹æ¡ˆå·²ç¡®å®š |

### 2.2 Proposal ä¼˜ç‚¹

1. **ç ”ç©¶ç©ºç™½è¯†åˆ«å‡†ç¡®**ï¼šç¡®å®æ²¡æœ‰åœ¨ spike-level tokenization + PerceiverIO ä¸Šåš masking é¢„è®­ç»ƒ + å›¾åƒé‡å»ºçš„å·¥ä½œ
2. **æŠ€æœ¯æ–¹æ¡ˆè®¾è®¡å‘¨åˆ°**ï¼šPre-PerceiverIO maskingã€ä¸ä½¿ç”¨ prompt tokenã€neuronÃ—time grid é‡å»ºç­‰è®¾è®¡å†³ç­–æœ‰å……åˆ†ç†ç”±
3. **Target Sequence Decoder è®¾è®¡åˆç†**ï¼šé¿å…äº† spike token çº§åˆ« masking çš„ä¿¡æ¯æ³„éœ²é—®é¢˜
4. **ä¸¤é˜¶æ®µè§†è§‰å¯¹é½ç­–ç•¥æˆç†Ÿ**ï¼šå€Ÿé‰´ MindEye2 çš„ Contrastive + Diffusion è·¯çº¿

### 2.3 éœ€è¦ä¿®æ­£çš„å£°æ˜

- "spike-level tokenization" â†’ åº”æ”¹ä¸º "event-level tokenization with continuous value support"ï¼ˆå› ä¸º TVSD ç”¨çš„æ˜¯ MUAï¼Œä¸æ˜¯ spikeï¼‰
- "d_model=512, 12 layers" â†’ å®é™…èµ·æ­¥ dim=128, depth=6ï¼Œè®ºæ–‡ä¸­ä½“ç° scaling å®éªŒ
- Proposal ä¸­çš„ "4-8Ã— A100" â†’ å®é™…ç”¨å•å¡ RTX 4090 24GBï¼Œéœ€è¦è°ƒæ•´æ‰€æœ‰ batch size å’Œ gradient accumulation

---

## 3. å…³é”®æ¶æ„å†³ç­–

### å†³ç­– 1ï¼šMUA æ•°æ®å¤„ç†æ–¹å¼

**é€‰æ‹©**ï¼šCaPOYO-style è¿ç»­å€¼ tokenization

```python
# å‚è€ƒ capoyo.py:68
self.input_value_map = nn.Linear(1, dim // 2)  # MUA å€¼ â†’ åŠç»´åº¦åµŒå…¥
self.unit_emb = InfiniteVocabEmbedding(dim // 2)  # ç”µæ ID â†’ åŠç»´åº¦åµŒå…¥
# æ‹¼æ¥ä¸ºå®Œæ•´ token: [value_emb || unit_emb] â†’ dim ç»´

# å¯¹äº spike-sorted æ•°æ® (Allen/IBL): value = 1.0
# å¯¹äº MUA æ•°æ® (TVSD): value = MUA amplitude
```

**ç†ç”±**ï¼šCaPOYO å·²éªŒè¯æ­¤æ–¹æ¡ˆå¯å¤„ç†é’™æˆåƒè¿ç»­ä¿¡å·ï¼Œç›´æ¥å¤ç”¨ä»£ç ã€‚

### å†³ç­– 2ï¼šæ¨¡å‹è§„æ¨¡

**é€‰æ‹©**ï¼šdim=128, depth=6 (~5M params)

**ç†ç”±**ï¼šRTX 4090 24GB åœ¨ AMP ä¸‹å¯è®­ç»ƒæ­¤è§„æ¨¡ã€‚POYO å·²åœ¨ dim=64 å’Œ dim=128 ä¸ŠéªŒè¯ã€‚

### å†³ç­– 3ï¼šé‡å»º Loss

**é€‰æ‹©**ï¼š
- Spike count targets (Allen): Poisson NLL
- MUA targets (TVSD): MSE Loss

**ç†ç”±**ï¼šåŒ¹é…æ•°æ®ç»Ÿè®¡ç‰¹æ€§ã€‚`torch_brain/nn/loss.py` å·²æœ‰ MSE å®ç°ã€‚

### å†³ç­– 4ï¼šGrid æ—¶é—´åˆ†è¾¨ç‡

**é€‰æ‹©**ï¼š50msï¼ˆ= latent_step çš„ä¸€åŠæˆ–ç­‰äºï¼‰

**ç†ç”±**ï¼š10ms grid å¯¹ 100 ä¸ªç¥ç»å…ƒäº§ç”Ÿ 10,000 ä¸ª query positionsï¼Œè®¡ç®—é‡è¿‡å¤§ã€‚50ms ä¸ backbone å‹ç¼©ç²’åº¦æ¥è¿‘ï¼Œæ›´åˆç†ã€‚

### å†³ç­– 5ï¼šæ•°æ®ä¼˜å…ˆçº§

**é€‰æ‹©**ï¼šTVSDï¼ˆæ ¸å¿ƒï¼‰> Allenï¼ˆé¢„è®­ç»ƒéªŒè¯ï¼‰>> IBLï¼ˆæš‚ä¸ç”¨ï¼‰

**ç†ç”±**ï¼šå•å¡èµ„æºæœ‰é™ï¼ŒTVSD æ˜¯å›¾åƒé‡å»ºçš„å”¯ä¸€å¯é æ•°æ®æºã€‚

---

## 4. æ‰§è¡Œè®¡åˆ’

### 4.0 å½“å‰èµ„æºçº¦æŸ

| èµ„æº | å®é™…æƒ…å†µ | å½±å“ |
|------|---------|------|
| GPU | RTX 4090 D 24GB | æ¨¡å‹ â‰¤ 30M paramsï¼Œbatch_size â‰¤ 64ï¼Œéœ€ AMP |
| å­˜å‚¨ | /root/autodl-tmp/ | éœ€ç¡®è®¤å¯ç”¨ç©ºé—´ï¼ˆTVSD MUA ~200-500GBï¼‰ |
| TVSD | æœªä¸‹è½½ | å…³é”®è·¯å¾„ï¼Œéœ€ç«‹å³å¯åŠ¨ |
| JiaLab æ•°æ® | ä¸å¯ç”¨ | ä»è®¡åˆ’ä¸­ç§»é™¤ |

### 4.1 Phase 0: ç¯å¢ƒæ­å»º + æ•°æ®å¯åŠ¨

**ç›®æ ‡**ï¼šå¤ç° POYO baseline + å¯åŠ¨æ•°æ®ä¸‹è½½

| å­ä»»åŠ¡ | æè¿° | éªŒæ”¶æ ‡å‡† |
|--------|------|---------|
| 0.1 | å®‰è£… torch_brain åŠä¾èµ– | `import torch_brain` æˆåŠŸ |
| 0.2 | ä¸‹è½½ NLB MC_Maze æ•°æ® | æ•°æ®æ–‡ä»¶å­˜åœ¨ä¸”å¯åŠ è½½ |
| 0.3 | è¿è¡Œ POYO baseline è®­ç»ƒ | velocity RÂ² â‰ˆ 0.87 |
| 0.4 | å¯åŠ¨ TVSD datalad clone | ä»“åº“å…‹éš†æˆåŠŸ |
| 0.5 | ä¸‹è½½ Allen å‰ 5 sessions | NWB æ–‡ä»¶å¯åŠ è½½ |

### 4.2 Phase 1a: TVSD æ•°æ®é€‚é… + ç»Ÿä¸€ Tokenizer

**ç›®æ ‡**ï¼šTVSD MUA æ•°æ®èƒ½é€šè¿‡ CaPOYO-style å‰å‘ä¼ æ’­

**æ–°å¢æ–‡ä»¶**ï¼š
- `neurobridge/data/tvsd_loader.py`
- `neurobridge/data/tvsd_dataset.py`

**GO/NO-GO å†³ç­–ç‚¹ 1**ï¼šCaPOYO tokenization æ˜¯å¦æœ‰æ•ˆå¤„ç† TVSD MUAï¼Ÿ

### 4.3 Phase 1b: Masking é¢„è®­ç»ƒ

**ç›®æ ‡**ï¼šåœ¨ Allen ä¸ŠéªŒè¯ masking é¢„è®­ç»ƒäº§ç”Ÿæœ‰æ„ä¹‰çš„è¡¨å¾

**æ–°å¢æ–‡ä»¶**ï¼š
- `neurobridge/decoders/target_seq_decoder.py`
- `neurobridge/masking/strategies.py` (temporal + neuron)
- `neurobridge/masking/controller.py`
- `neurobridge/losses/reconstruction_loss.py`

**æ¨¡å‹é…ç½®**ï¼š
```yaml
dim: 128
depth: 6
dim_head: 64
cross_heads: 2
self_heads: 4
num_latents_per_step: 8
latent_step: 0.125
sequence_length: 1.0
```

**éªŒæ”¶æ ‡å‡†**ï¼š
- é‡å»º loss æŒç»­ä¸‹ï¿½ï¿½
- Linear probe RÂ² > éšæœºåˆå§‹åŒ–

**GO/NO-GO å†³ç­–ç‚¹ 2**ï¼šé¢„è®­ç»ƒæ˜¯å¦äº§ç”Ÿæœ‰æ„ä¹‰çš„è¡¨å¾ï¼Ÿ

### 4.4 Phase 2: CLIP å¯¹é½ + å›¾åƒé‡å»º

**ç›®æ ‡**ï¼šä» TVSD MUA æ•°æ®é‡å»ºå¯è¾¨è¯†çš„å›¾åƒ

**æ–°å¢æ–‡ä»¶**ï¼š
- `neurobridge/alignment/readout.py`ï¼ˆK=8 learnable queriesï¼‰
- `neurobridge/alignment/projector.py`ï¼ˆ3 å±‚ MLP â†’ 768 ç»´ CLIP ç©ºé—´ï¼‰
- `neurobridge/alignment/infonce.py`
- `neurobridge/alignment/clip_wrapper.py`
- `neurobridge/generation/diffusion_adapter.py`
- `neurobridge/generation/sd_wrapper.py`
- `scripts/evaluate_reconstruction.py`

**ä¸¤é˜¶æ®µå¯¹é½**ï¼š
1. Stage 1: å†»ç»“ encoder + å†»ç»“ CLIPï¼Œè®­ç»ƒ readout + projector (InfoNCE)
2. Stage 2: è§£å†» encoder æœ€å 2-3 å±‚ï¼Œè”åˆ InfoNCE + anti-forgetting loss

**éªŒæ”¶æ ‡å‡†**ï¼š
- CLIP retrieval Top-5 > 15% (Stage 1) / > 30% (Stage 2)
- é‡å»ºå›¾åƒè§†è§‰ä¸Šå¯è¾¨è¯†

**GO/NO-GO å†³ç­–ç‚¹ 3**ï¼šé‡å»ºå›¾åƒæ˜¯å¦æœ‰æ„ä¹‰ï¼Ÿ

### 4.5 Phase 3: æ¶ˆèå®éªŒ

**ä¼˜å…ˆçº§**ï¼ˆä»é«˜åˆ°ä½ï¼‰ï¼š
1. âœ… å¿…åšï¼šé¢„è®­ç»ƒ vs éšæœºåˆå§‹åŒ– encoder
2. âœ… å¿…åšï¼šV1 vs V4 vs IT vs V1+V4+IT è„‘åŒºè´¡çŒ®
3. âœ… å¿…åšï¼štemporal masking vs neuron masking vs combined
4. ğŸ”µ å¯åšï¼šä¸åŒæ—¶é—´çª—å£å¯¹å„è„‘åŒºçš„æ•ˆæœ
5. ğŸ”µ å¯åšï¼šdim=128 vs dim=256 scaling
6. â¬œ å»¶åï¼šè·¨ session æ³›åŒ–
7. â¬œ å»¶åï¼šIBL æ•°æ®æ··åˆ

### 4.6 Phase 4: è®ºæ–‡æ’°å†™

- ç»“æœå¯è§†åŒ–
- NeurIPS/ICML æ ¼å¼åˆç¨¿
- å®¡é˜…ä¿®æ”¹

---

## 5. é£é™©ç™»è®°è¡¨

| é£é™© | æ¦‚ç‡ | å½±å“ | åº”å¯¹ç­–ç•¥ |
|------|------|------|---------|
| TVSD ä¸‹è½½è€—æ—¶/å¤±è´¥ | é«˜ | é˜»å¡å›¾åƒé‡å»º | ç«‹å³å¯åŠ¨ï¼›ç”¨ Allen 118 å¼ å›¾åšåŸå‹ |
| 24GB VRAM ä¸å¤Ÿ | ä¸­ | é™åˆ¶æ¨¡å‹/batch å¤§å° | AMP + gradient checkpoint + accumulation |
| MUA tokenization æ•ˆæœå·® | ä½-ä¸­ | TVSD ä¸å¯ç”¨ | å›é€€åˆ° binned representation |
| é¢„è®­ç»ƒå¯¹é‡å»ºæ— å¸®åŠ© | ä¸­ | æ ¸å¿ƒ claim ä¸æˆç«‹ | æ”¹å‘ "direct alignment" è®ºæ–‡ |
| CLIP å¯¹é½å¤±è´¥ | ä½-ä¸­ | æ— é‡å»ºç»“æœ | å°è¯• DINOv2ï¼›å¢åŠ  projector å®¹é‡ |

---

## 6. æœ€å°å¯å‘è¡¨è®ºæ–‡æ–¹æ¡ˆ

### æ–¹æ¡ˆ Aï¼ˆæœ€ä¼˜ï¼‰ï¼šå®Œæ•´ NeuroBridge
- é¢„è®­ç»ƒï¼ˆAllen + TVSDï¼‰ + CLIP å¯¹é½ï¼ˆTVSDï¼‰ + å›¾åƒé‡å»ºï¼ˆTVSDï¼‰
- æ¶ˆèå®éªŒè¯æ˜é¢„è®­ç»ƒä»·å€¼ + è„‘åŒºè´¡çŒ®åˆ†æ
- ç›®æ ‡ï¼šNeurIPS / ICML

### æ–¹æ¡ˆ Bï¼ˆé™çº§ï¼‰ï¼šMasked Pretraining Only
- å¦‚æœå›¾åƒé‡å»ºæ•ˆæœä¸ä½³ï¼Œä»…å‘è¡¨é¢„è®­ç»ƒç»„ä»¶
- åœ¨ Allen/IBL ä¸ŠéªŒè¯ masking é¢„è®­ç»ƒçš„è¡¨å¾è´¨é‡
- ç›®æ ‡ï¼šWorkshop / ICLR

### æ–¹æ¡ˆ Cï¼ˆæœ€å°ï¼‰ï¼šDirect Neural-to-CLIP Alignment
- è·³è¿‡é¢„è®­ç»ƒï¼Œç›´æ¥åœ¨ TVSD ä¸Šåš MUA â†’ CLIP å¯¹é½
- ä¸ MonkeySee baseline å¯¹æ¯”
- ç›®æ ‡ï¼šè®¡ç®—ç¥ç»ç§‘å­¦ venue

---

## 7. è¿›åº¦è®°å½•

| æ—¥æœŸ | é˜¶æ®µ | å®Œæˆå†…å®¹ | é‡åˆ°çš„é—®é¢˜ | è§£å†³æ–¹æ¡ˆ |
|------|------|---------|-----------|---------|
| 2026-02-21 | é¡¹ç›®å¯åŠ¨ | âœ… POYO ä»£ç åˆ†æå®Œæˆ | PyTorch ç¯å¢ƒæœªå®‰è£… | å¾…å®‰è£… |
| 2026-02-21 | é¡¹ç›®å¯åŠ¨ | âœ… NeuroBridge proposal å®¡æŸ¥å®Œæˆ | GPU ä¸º 4090 24GB è€Œé A100 80GB | è°ƒæ•´æ¨¡å‹é…ç½®ä¸º dim=128 |
| 2026-02-21 | é¡¹ç›®å¯åŠ¨ | âœ… æ‰§è¡Œè®¡åˆ’åˆ¶å®šå®Œæˆ | TVSD æœªä¸‹è½½ | éœ€ç«‹å³å¯åŠ¨ |
| 2026-02-21 | é¡¹ç›®å¯åŠ¨ | âœ… åˆ†ææ–‡æ¡£ä¿å­˜åˆ° cc_todo | - | - |
| 2026-02-21 | Phase 0.1 | âœ… ç¯å¢ƒæ£€æŸ¥å®Œæˆ | torch_brain éœ€è®¾ PYTHONPATH | `PYTHONPATH=/root/autodl-tmp/NeuroBridge` |
| 2026-02-21 | Phase 0.1 | poyo conda ç¯å¢ƒç¡®è®¤å¯ç”¨ | PyTorch 2.10.0+cu128, RTX 4090D 25.3GB | - |
| 2026-02-21 | Phase 0.2 | âœ… 50 epoch å¿«é€ŸéªŒè¯é€šè¿‡ (RÂ²=-0.001) | æ­£å¸¸ â€” 1000 epoch æ‰æ”¶æ•› | åå°è¿è¡Œå®Œæ•´è®­ç»ƒ PID:9140 |
| 2026-02-21 | Phase 0.2 | âœ… 1000 epoch å®Œæ•´è®­ç»ƒå®Œæˆ | æœ€ç»ˆ test RÂ²=0.836 (ç›®æ ‡â‰ˆ0.87) | å¯æ¥å—ï¼Œcheckpoint åœ¨ epoch=799 |
| 2026-02-21 | Phase 0.3 | âœ… TVSD æ•°æ®ä»“åº“å…‹éš†æˆåŠŸ | datalad éœ€è¦ git-annex â‰¥ 10.x | `conda install -c conda-forge git-annex` |
| 2026-02-21 | Phase 0.3 | âœ… normMUA.mat ä¸‹è½½å®Œæˆ (ä¸¤åªçŒ´å„~194MB) | MAT v7.3 éœ€ç”¨ h5py è¯»å– | scipy.io.loadmat ä¸æ”¯æŒ v7.3 |
| 2026-02-21 | Phase 0.3 | âœ… TVSD æ•°æ®ç»“æ„å®Œæ•´æ¢ç´¢ | normMUA æ˜¯æ—¶é—´å¹³å‡çš„ 2D æ•°æ® [22248,1024] | å¯¹ CLIP å¯¹é½å·²è¶³å¤Ÿ |
| 2026-02-21 | Phase 0.3 | âœ… ç”µæ-è„‘åŒºæ˜ å°„ç¡®è®¤ | æ˜ å°„æ¥è‡ª norm_MUA.m æºä»£ç  | è§ä¸‹æ–¹è¯¦ç»†è®°å½• |
| 2026-02-21 | Phase 0.3 | âœ… å›¾åƒ-è¯•æ¬¡æ˜ å°„ç¡®è®¤ | things_imgs.mat åŒ…å« THINGS è·¯å¾„ | train:22248å¼ , test:100å¼  |
| 2026-02-21 | Phase 1a | âœ… NeuroBridge åŒ…ç»“æ„ + æ•°æ®é€‚é… | - | neurobridge/{data,models,tests}/ |
| 2026-02-21 | Phase 1a | âœ… å‰å‘ä¼ æ’­éªŒè¯é€šè¿‡ | 2.3M params, 143.5MB GPU | 5/5 tests passed |
| 2026-02-21 | Phase 2a | âœ… CLIP å¯¹é½æ¨¡å—å®ç° | - | CLIPWrapper, Readout, Projector, InfoNCE |
| 2026-02-21 | Phase 2a | âœ… Pipeline éªŒè¯ï¼ˆéšæœºåµŒå…¥ï¼‰ | ~7s/epoch, 3.3M params | chance level æ­£å¸¸ |
| 2026-02-21 | Phase 2b | âœ… THINGS å›¾åƒä¸‹è½½å®Œæˆ | HF ç›´è¿ä¸å¯ç”¨ | hf-mirror.com é•œåƒ |
| 2026-02-21 | Phase 2b | âœ… CLIP åµŒå…¥æå–å®Œæˆ | 22248+100 å¼ , 0 missing | ViT-L-14 (768-dim) |
| 2026-02-21 | Phase 2b | âœ… diffusers å®‰è£…å®Œæˆ | diffusers 0.36.0 | - |
| 2026-02-21 | Phase 2b | ğŸ”„ CLIP å¯¹é½è®­ç»ƒå¯åŠ¨ | 100 epochs, åå°è¿è¡Œ | å¾…æŸ¥çœ‹ç»“æœ |
| 2026-02-21 | Phase 2b | âœ… CLIP å¯¹é½è®­ç»ƒå®Œæˆ | **Test Top-1=53%, Top-5=82%** | Best model at epoch 40 |
| 2026-02-21 | Phase 2b+ | âœ… V2 è®­ç»ƒï¼ˆå¸¦å¢å¼ºï¼‰å®Œæˆ | **Test Top-5=85-89%**, val top5=62.8% | ç”µædropout+å™ªå£°æœ‰æ•ˆ |
| 2026-02-21 | Phase 2c | âœ… DiffusionAdapter å®ç° | SD 2.1â†’1.5 (768-dim), å·²ç¼“å­˜ | reconstruct_images.py |
| 2026-02-21 | Phase 2c | âœ… DiffusionAdapter å®ç° | Token Expander + Refiner | 768â†’77Ã—1024 |
| | | | | |

---

## 8. TVSD æ•°æ®ç»“æ„è¯¦ç»†è®°å½•

### 8.1 ç”µæ-è„‘åŒºæ˜ å°„

**æ¥æº**ï¼š`_code/norm_MUA.m` ç¬¬ 11-19 è¡Œ

| çŒ´å­ | é€šé“èŒƒå›´ | è„‘åŒº | ç”µææ•° |
|------|---------|------|--------|
| monkeyF | 1-512 | V1 | 512 |
| monkeyF | 513-832 | IT | 320 |
| monkeyF | 833-1024 | V4 | 192 |
| monkeyN | 1-512 | V1 | 512 |
| monkeyN | 513-768 | V4 | 256 |
| monkeyN | 769-1024 | IT | 256 |

**æ³¨æ„**ï¼š`1024chns_mapping_20220105.mat` åŒ…å«é€šé“é‡æ’æ˜ å°„ï¼ˆrecording system â†’ physical orderï¼‰ã€‚`norm_MUA.m` ä¸­å…ˆå®šä¹‰ roisï¼Œå† `rois = rois(mapping)` é‡æ’ï¼Œæœ€ç»ˆ normMUA æ•°æ®å·²æŒ‰ç‰©ç†é¡ºåºå­˜å‚¨ã€‚

### 8.2 æ—¶é—´çª—å£ï¼ˆç”¨äº normMUA æ—¶é—´å¹³å‡ï¼‰

| è„‘åŒº | æ—¶é—´çª—å£ (ms post-stimulus) |
|------|---------------------------|
| V1 | 25-125 |
| V4 | 50-150 |
| IT | 75-175 |

### 8.3 normMUA æ•°æ®æ ¼å¼

```
THINGS_normMUA.mat (h5py):
  train_MUA: [1024, 22248] â†’ è½¬ç½®å [22248, 1024]
  test_MUA:  [1024, 100]   â†’ è½¬ç½®å [100, 1024]
  test_MUA_reps: [1024, 100, 30] â†’ è½¬ç½®å [30, 100, 1024]
  tb: [-100ms to +199ms], 300 time bins at 1ms
  SNR, SNR_max, lats, oracle, reliab: è´¨é‡æŒ‡æ ‡
```

**å…³é”®å‘ç°**ï¼šnormMUA æ˜¯**æ—¶é—´å¹³å‡çš„ 2D æ•°æ®**ï¼ˆæ¯ä¸ªç”µææ¯å¼ å›¾ä¸€ä¸ªæ ‡é‡å€¼ï¼‰ï¼Œä¸å«æ—¶é—´ç»´åº¦ã€‚è¿™å¯¹ CLIP å¯¹é½è¶³å¤Ÿï¼Œä½†åš masking é¢„è®­ç»ƒéœ€è¦å®Œæ•´æ—¶é—´åºåˆ— `THINGS_MUA_trials.mat`ï¼ˆ~58GB/çŒ´ï¼‰ã€‚

### 8.4 å›¾åƒ-è¯•æ¬¡æ˜ å°„

```
things_imgs.mat (h5py):
  train_imgs:
    class: [22248, 1] â†’ å›¾åƒç±»åˆ«å (e.g., "aardvark")
    things_path: [22248, 1] â†’ THINGS è·¯å¾„ (e.g., "aardvark/aardvark_01b.jpg")
    local_path: [22248, 1]
  test_imgs:
    class: [100, 1]
    things_path: [100, 1]
    local_path: [100, 1]
```

normMUA ä¸­çš„ train_MUA/test_MUA å·²æŒ‰ things_imgs æ’åºï¼Œç›´æ¥å¯¹åº”ã€‚

### 8.5 å¯¹ NeuroBridge pipeline çš„å½±å“

1. **CLIP å¯¹é½ï¼ˆPhase 2ï¼‰å¯ç›´æ¥ä½¿ç”¨ normMUA**ï¼š22248 ä¸ª trial Ã— 1024 é€šé“ï¼Œæ¯ä¸ªå¯¹åº”ä¸€å¼  THINGS å›¾åƒ
2. **Masking é¢„è®­ç»ƒï¼ˆPhase 1bï¼‰éœ€è¦å®Œæ•´æ—¶é—´åºåˆ—**ï¼šéœ€ä¸‹è½½ THINGS_MUA_trials.matï¼ˆ~58GBï¼‰ï¼Œæˆ–åœ¨ normMUA ä¸Šè®¾è®¡æ›¿ä»£é¢„è®­ç»ƒæ–¹æ¡ˆ
3. **è„‘åŒºæ¶ˆèå®éªŒï¼ˆPhase 3ï¼‰æœ‰æ˜ç¡®é€šé“åˆ’åˆ†**ï¼šå¯ç›´æ¥é€‰å– V1/V4/IT å­é›†

---

## 9. Phase 1a å®ç°è®°å½•

### 9.1 å®Œæˆå†…å®¹

| æ—¥æœŸ | å®Œæˆå†…å®¹ | å…³é”®æ•°æ® |
|------|---------|---------|
| 2026-02-21 | âœ… neurobridge åŒ…ç»“æ„åˆ›å»º | `neurobridge/{data,models,tests}/` |
| 2026-02-21 | âœ… TVSDNormMUADataset å®ç° | æ”¯æŒ raw/capoyo ä¸¤ç§æ¨¡å¼ï¼Œè„‘åŒºè¿‡æ»¤ï¼ŒSNR è¿‡æ»¤ |
| 2026-02-21 | âœ… NeuroBridgeEncoder å®ç° | åŸºäº CaPOYO æ¶æ„ï¼Œdim=128, depth=6, 2.3M params |
| 2026-02-21 | âœ… å‰å‘ä¼ æ’­éªŒè¯é€šè¿‡ | 5/5 æµ‹è¯•é€šè¿‡ |

### 9.2 å‰å‘ä¼ æ’­éªŒè¯ç»“æœ

```
Dataset: 22248 train, 100 test, 1024 electrodes, 1854 classes
Model: NeuroBridgeEncoder, 2,308,032 parameters
Input: (batch=32, 1024 tokens, 1 value) â†’ forward â†’ (32, 8, 128) latent output
GPU memory: 143.5 MB peak (batch_size=32, AMP)
MUA stats: mean=0.020, std=0.443, range=[-2.59, 71.0]
```

### 9.3 æ–°å¢æ–‡ä»¶æ¸…å•

```
neurobridge/__init__.py
neurobridge/data/__init__.py
neurobridge/data/tvsd_dataset.py          # TVSD normMUA dataset adapter
neurobridge/models/__init__.py
neurobridge/models/neurobridge_encoder.py # CaPOYO-based neural encoder
neurobridge/tests/__init__.py
neurobridge/tests/test_tvsd_forward.py    # Forward pass verification
```

### 9.4 GO/NO-GO å†³ç­–ç‚¹ 1

**CaPOYO-style tokenization æ˜¯å¦æœ‰æ•ˆå¤„ç† TVSD MUAï¼Ÿ**

**ç»“è®ºï¼šGO** âœ…
- normMUA æ—¶é—´å¹³å‡æ•°æ®é€šè¿‡ CaPOYO tokenization æˆåŠŸ
- 1024 ç”µæ â†’ 1024 input tokens â†’ 8 latent tokens â†’ 128-dim representations
- GPU å†…å­˜å¼€é”€æä½ï¼ˆbatch=32 ä»… 144MBï¼‰
- ä¸‹ä¸€æ­¥ï¼šPhase 1b (masking é¢„è®­ç»ƒ) æˆ–ç›´æ¥è·³åˆ° Phase 2 (CLIP å¯¹é½)

---

## 10. Phase 2a å®ç°è®°å½•ï¼šCLIP å¯¹é½æ¨¡å—

### 10.1 å®Œæˆå†…å®¹

| æ—¥æœŸ | å®Œæˆå†…å®¹ | å…³é”®æ•°æ® |
|------|---------|---------|
| 2026-02-21 | âœ… å®‰è£… transformers + open_clip_torch | transformers 5.2.0, open_clip 3.2.0 |
| 2026-02-21 | âœ… CLIP å¯¹é½æ¨¡å—å®ç° | CLIPWrapper, NeuralReadout, NeuralProjector, InfoNCELoss |
| 2026-02-21 | âœ… ç«¯åˆ°ç«¯è®­ç»ƒè„šæœ¬å®ç° | train_clip_alignment.py |
| 2026-02-21 | âœ… Pipeline éªŒè¯é€šè¿‡ï¼ˆéšæœº CLIP embeddingsï¼‰ | 5 epochs, ~7s/epoch |

### 10.2 æ¨¡å—æ¶æ„

```
TVSD normMUA [B, 1024]
    â†’ NeuroBridgeEncoder (CaPOYO-style)
        1024 input tokens â†’ PerceiverIO â†’ 8 latent tokens Ã— 128 dim
    â†’ NeuralReadout (cross-attention)
        8 learnable queries attend to 8 latents â†’ 8 readout tokens Ã— 128 dim
    â†’ NeuralProjector (3-layer MLP)
        mean pool â†’ 512 hidden â†’ 768 output (CLIP dim)
        â†’ L2 normalize
    â†’ InfoNCE loss â† CLIP image embedding (768-dim)
```

### 10.3 Pipeline éªŒè¯ç»“æœï¼ˆéšæœº CLIP åµŒå…¥ï¼‰

```
Model: 3,299,648 params
Epoch 1: loss=4.917, acc=0.007, top5=0.039 (chance: 1/128=0.008)
Epoch 5: loss=4.880, acc=0.009, top5=0.045
Speed: ~7s/epoch (22248 samples, batch=128, RTX 4090)
ç»“è®º: pipeline è¿è¡Œæ­£ç¡®ï¼ŒæŒ‡æ ‡åœ¨ chance levelï¼ˆç¬¦åˆé¢„æœŸï¼‰
```

### 10.4 æ–°å¢æ–‡ä»¶æ¸…å•

```
neurobridge/alignment/__init__.py
neurobridge/alignment/clip_wrapper.py    # CLIP æ¨¡å‹å°è£… (open_clip)
neurobridge/alignment/readout.py         # å¯å­¦ä¹  readout æŸ¥è¯¢
neurobridge/alignment/projector.py       # MLP projector â†’ CLIP ç©ºé—´
neurobridge/alignment/infonce.py         # å¯¹ç§° InfoNCE loss
scripts/train_clip_alignment.py          # ç«¯åˆ°ç«¯è®­ç»ƒè„šæœ¬
```

### 10.5 ä¸‹ä¸€æ­¥ï¼šè·å–çœŸå® CLIP åµŒå…¥

**é—®é¢˜**ï¼šTHINGS å›¾åƒéœ€ä» OSF (https://osf.io/jum2f/) ä¸‹è½½
**æ–¹æ¡ˆ**ï¼š
1. ä¸‹è½½ THINGS å›¾åƒæ•°æ®åº“ï¼ˆ~5GBï¼Œ26107 å¼ å›¾ï¼‰â†’ æ­£åœ¨ä¸‹è½½ä¸­
2. ç”¨ open_clip ViT-L-14 é¢„æå–æ‰€æœ‰ 22248+100 å¼ å›¾çš„ CLIP åµŒå…¥
3. ä¿å­˜ä¸º .npy æ–‡ä»¶ï¼Œä¾›è®­ç»ƒæ—¶ç›´æ¥åŠ è½½

---

## 11. Phase 2c å®ç°è®°å½•ï¼šDiffusion Adapter

### 11.1 å®Œæˆå†…å®¹

| æ—¥æœŸ | å®Œæˆå†…å®¹ | å…³é”®æ•°æ® |
|------|---------|---------|
| 2026-02-21 | âœ… DiffusionAdapter å®ç° | Token Expander + Refiner (768 â†’ 77Ã—1024) |
| 2026-02-21 | âœ… StableDiffusionWrapper å®ç° | SD 2.1 + DDIM 50 steps |
| 2026-02-21 | âœ… CLIP åµŒå…¥æå–è„šæœ¬ | scripts/extract_clip_embeddings.py |
| 2026-02-21 | âœ… è¯„ä¼°è„šæœ¬ | scripts/evaluate_alignment.py |
| 2026-02-21 | ğŸ”„ THINGS å›¾åƒä¸‹è½½ä¸­ | images_THINGS.zip (~5GB from OSF, password: things4all) |

### 11.2 ç«¯åˆ°ç«¯ pipeline å®Œæ•´æ¶æ„

```
Phase 1a: TVSD normMUA [22248, 1024]
    â†“
Phase 2a: NeuroBridgeEncoder (CaPOYO-style, 2.3M params)
    1024 electrodes â†’ 1024 tokens â†’ PerceiverIO â†’ 8 latents Ã— 128d
    â†“
Phase 2a: NeuralReadout (8 learnable queries, cross-attention)
    â†“
Phase 2a: NeuralProjector (3-layer MLP â†’ 768-dim CLIP space)
    â†“
Phase 2a: InfoNCE loss â†â†’ CLIP image embeddings (768-dim)
    â†“ (after training)
Phase 2c: DiffusionAdapter (cross-attn + self-attn refiner)
    768-dim â†’ 77 tokens Ã— 1024-dim (SD conditioning)
    â†“
Phase 2c: StableDiffusionWrapper (SD 2.1 + DDIM)
    â†’ reconstructed image (512Ã—512)
```

### 11.3 æ–°å¢æ–‡ä»¶æ¸…å•

```
neurobridge/generation/__init__.py
neurobridge/generation/diffusion_adapter.py  # DiffusionAdapter + SDWrapper
scripts/extract_clip_embeddings.py           # CLIP åµŒå…¥é¢„æå–
scripts/evaluate_alignment.py                # æ£€ç´¢è¯„ä¼°æŒ‡æ ‡
```

### 11.4 å¾…å®Œæˆ

1. âœ… THINGS å›¾åƒä¸‹è½½å®Œæˆåæå– CLIP åµŒå…¥
2. ğŸ”„ ç”¨çœŸå® CLIP åµŒå…¥è®­ç»ƒå¯¹é½æ¨¡å‹ï¼ˆæ­£åœ¨åå°è¿è¡Œï¼‰
3. æµ‹è¯• DiffusionAdapter + SD ç”Ÿæˆ
4. âœ… å®‰è£… diffusers åŒ…

---

## 12. Phase 2b å®ç°è®°å½•ï¼šCLIP åµŒå…¥æå– + çœŸå®å¯¹é½è®­ç»ƒ

### 12.1 å®Œæˆå†…å®¹

| æ—¥æœŸ | å®Œæˆå†…å®¹ | å…³é”®æ•°æ® |
|------|---------|---------|
| 2026-02-21 | âœ… THINGS å›¾åƒä¸‹è½½å®Œæˆ | 4.68GB, 1854 class folders, OSF password: things4all |
| 2026-02-21 | âœ… CLIP åµŒå…¥æå–å®Œæˆ | ViT-L-14 (openai), 22248 train + 100 test, 0 missing |
| 2026-02-21 | âœ… diffusers å®‰è£…å®Œæˆ | diffusers 0.36.0 |
| 2026-02-21 | ğŸ”„ CLIP å¯¹é½è®­ç»ƒå¯åŠ¨ | 100 epochs, batch=256, lr=3e-4, 3.3M params |

### 12.2 CLIP åµŒå…¥æå–

```
HuggingFace ç›´è¿ä¸å¯ç”¨ â†’ ä½¿ç”¨ HF_ENDPOINT=https://hf-mirror.com é•œåƒ
ViT-L-14 (openai pretrained) â†’ 768-dim embeddings
Train: 22248 images â†’ clip_train_monkeyF.npy (22248, 768)
Test:  100 images   â†’ clip_test_monkeyF.npy  (100, 768)
æ‰€æœ‰å›¾åƒè·¯å¾„åŒ¹é…æˆåŠŸï¼Œ0 ä¸ªç¼ºå¤±
```

### 12.3 CLIP å¯¹é½è®­ç»ƒé…ç½®

```json
{
  "monkey": "monkeyF",
  "epochs": 100,
  "batch_size": 256,
  "lr": 3e-4,
  "weight_decay": 1e-4,
  "val_ratio": 0.1,
  "encoder_dim": 128,
  "encoder_depth": 6,
  "num_latents": 8,
  "clip_dim": 768,
  "projector_hidden": 512,
  "n_params": 3299648,
  "n_train": 20024,
  "n_val": 2224
}
```

### 12.4 è®­ç»ƒç»“æœ

**è®­ç»ƒå®Œæˆï¼100 epochs, å…± ~400s (~4s/epoch)**

#### å…³é”®æŒ‡æ ‡èµ°åŠ¿

| é˜¶æ®µ | è®­ç»ƒ Loss | è®­ç»ƒ Acc | éªŒè¯ Loss | éªŒè¯ Acc | éªŒè¯ Top-5 |
|------|----------|---------|----------|---------|-----------|
| Epoch 1 | 5.423 | 0.8% | 5.329 | 1.0% | 4.4% |
| Epoch 10 | 4.272 | 7.7% | 4.267 | 7.8% | 26.8% |
| Epoch 20 | 3.356 | 22.3% | 3.599 | 18.3% | 45.6% |
| Epoch 30 | 2.425 | 41.8% | 3.250 | 23.9% | 53.6% |
| **Epoch 40 (best)** | **1.514** | **64.5%** | **3.164** | **26.6%** | **56.0%** |
| Epoch 60 | 0.399 | 94.1% | 3.392 | 26.5% | 53.3% |
| Epoch 100 | 0.118 | 98.8% | 3.628 | 25.3% | 51.2% |

**è¿‡æ‹Ÿåˆåˆ†æ**ï¼šè®­ç»ƒä» epoch 35-40 å¼€å§‹ä¸¥é‡è¿‡æ‹Ÿåˆã€‚Best model saved at epoch 40.

#### æµ‹è¯•é›†è¯„ä¼°ç»“æœ (100 å¼ å›¾åƒ)

```
Neural â†’ Image:
  Top-1:  53.0%  (chance: 1.0%)
  Top-5:  82.0%  (chance: 5.0%)
  Top-10: 94.0%  (chance: 10.0%)
  Top-50: 100.0%

Image â†’ Neural:
  Top-1:  54.0%
  Top-5:  81.0%
  Top-10: 93.0%
  Top-50: 100.0%

Similarity:
  Positive (meanÂ±std): 0.164 Â± 0.046
  Negative (meanÂ±std): 0.025 Â± 0.050
  Median rank: 0 (å³å¤§éƒ¨åˆ†æŸ¥è¯¢æ’åç¬¬ä¸€)
```

**GO/NO-GO å†³ç­–ç‚¹ 3**ï¼šâœ… **GO** â€” Top-5 retrieval accuracy 82% è¿œè¶… 15% é˜ˆå€¼ï¼

Checkpoint ä¿å­˜åœ¨ï¼š
```
checkpoints/clip_alignment_v1/
  config.json              # è®­ç»ƒé…ç½®
  best_model.pt            # Epoch 40, val top5=56.0%
  checkpoint_epoch{10-100}.pt  # æ¯ 10 epoch æ£€æŸ¥ç‚¹
  training_log.txt         # å®Œæ•´è®­ç»ƒæ—¥å¿—
  test_metrics.json        # æµ‹è¯•é›†è¯„ä¼°ç»“æœ
```

### 12.5 V2 è®­ç»ƒï¼ˆå¸¦å¢å¼ºï¼‰

**æ”¹è¿›**ï¼š
- ç”µæ dropout = 0.15ï¼ˆéšæœºä¸¢å¼ƒ 15% ç”µæï¼‰
- é«˜æ–¯å™ªå£° std = 0.1
- Weight decay 5e-4ï¼ˆä» 1e-4 å¢å¤§ï¼‰
- Early stopping patience = 30

**V2 ç»“æœ**ï¼š
- Best val top-5: **62.8%** (epoch 42, æå‡ +6.8%)
- Early stopping at epoch 72

**V2 æµ‹è¯•é›†è¯„ä¼° (100 å¼ å›¾åƒ)**ï¼š

| æŒ‡æ ‡ | V1 | V2 | å˜åŒ– |
|------|----|----|------|
| Nâ†’I Top-1 | 53.0% | 49.0% | -4% |
| Nâ†’I Top-5 | 82.0% | 85.0% | **+3%** |
| Iâ†’N Top-1 | 54.0% | 56.0% | +2% |
| Iâ†’N Top-5 | 81.0% | 89.0% | **+8%** |
| Top-10 | 94.0% | 94.0% | same |

**ç»“è®º**ï¼šV2 åœ¨ Top-5 ä¸Šæ›´å¼º (85-89%)ï¼ŒTop-1 ç•¥æœ‰æ³¢åŠ¨ï¼ˆ100 å¼ æµ‹è¯•é›†æ–¹å·®è¾ƒå¤§ï¼‰ã€‚å»ºè®®åç»­ä»¥ V2 ä¸ºåŸºç¡€ã€‚

### 12.6 æ•°æ®æ–‡ä»¶ä½ç½®

```
data/clip_embeddings/
  clip_train_monkeyF.npy  # (22248, 768) float32
  clip_test_monkeyF.npy   # (100, 768) float32

checkpoints/clip_alignment_v1/  # æ— å¢å¼º
checkpoints/clip_alignment_v2/  # å¸¦å¢å¼ºï¼ˆæ¨èï¼‰
```

### 12.7 SD æ¨¡å‹é€‚é…

**é—®é¢˜**ï¼šSD 2.1 (`stabilityai/stable-diffusion-2-1`) éœ€è¦ HF è®¤è¯ï¼Œé•œåƒä¸å¯ç”¨ã€‚
**è§£å†³**ï¼šæ”¹ç”¨ SD 1.5 (`runwayml/stable-diffusion-v1-5`)ï¼Œå·²ç¼“å­˜å¯ç”¨ã€‚
**å½±å“**ï¼šSD 1.5 text_hidden_dim=768ï¼ˆä¸ CLIP åµŒå…¥ç›¸åŒï¼‰ï¼ŒDiffusionAdapter éœ€è°ƒæ•´ sd_hidden_dimã€‚

---

## 13. å½“æ—¥æ€»ç»“ (2026-02-21)

### å®Œæˆçš„é˜¶æ®µ

| é˜¶æ®µ | çŠ¶æ€ | æ ¸å¿ƒæˆæœ |
|------|------|---------|
| Phase 0.1 | âœ… å®Œæˆ | ç¯å¢ƒæ­å»ºï¼Œtorch_brain å¯ç”¨ |
| Phase 0.2 | âœ… å®Œæˆ | POYO baseline RÂ²=0.836 |
| Phase 0.3 | âœ… å®Œæˆ | TVSD æ•°æ®ä¸‹è½½ + ç»“æ„æ¢ç´¢ |
| Phase 1a | âœ… å®Œæˆ | CaPOYO tokenization éªŒè¯é€šè¿‡ |
| Phase 2a | âœ… å®Œæˆ | CLIP å¯¹é½æ¨¡å—å…¨éƒ¨å®ç° |
| Phase 2b | âœ… å®Œæˆ | **Test Top-1=53%, Top-5=85%** |
| Phase 2c (éƒ¨åˆ†) | ğŸ”„ è¿›è¡Œä¸­ | DiffusionAdapter å®ç°å®Œæ¯•ï¼ŒSD 1.5 å¯ç”¨ |

### å½“æ—¥å…³é”®ä»£ç äº§å‡º

```
neurobridge/
  __init__.py
  data/
    __init__.py
    tvsd_dataset.py             # TVSD normMUA æ•°æ®é›†é€‚é…å™¨
  models/
    __init__.py
    neurobridge_encoder.py      # CaPOYO-based ç¼–ç å™¨ (2.3M params)
  alignment/
    __init__.py
    clip_wrapper.py             # CLIP å°è£…
    readout.py                  # å¯å­¦ä¹  readout æŸ¥è¯¢
    projector.py                # MLP æŠ•å½±å™¨ â†’ CLIP ç©ºé—´
    infonce.py                  # InfoNCE å¯¹æ¯”æŸå¤±
  generation/
    __init__.py
    diffusion_adapter.py        # DiffusionAdapter + SDWrapper
  tests/
    __init__.py
    test_tvsd_forward.py        # å‰å‘ä¼ æ’­æµ‹è¯•

scripts/
  train_clip_alignment.py       # CLIP å¯¹é½è®­ç»ƒè„šæœ¬
  extract_clip_embeddings.py    # CLIP åµŒå…¥æå–
  evaluate_alignment.py         # æ£€ç´¢è¯„ä¼°
  reconstruct_images.py         # ç«¯åˆ°ç«¯å›¾åƒé‡å»º

data/clip_embeddings/
  clip_train_monkeyF.npy        # (22248, 768)
  clip_test_monkeyF.npy         # (100, 768)
```

### æ¥ä¸‹æ¥è¦åšçš„

1. **Phase 2c æ”¶å°¾**ï¼šè®­ç»ƒ DiffusionAdapter + SD ç”Ÿæˆé¦–æ‰¹é‡å»ºå›¾åƒ
2. **Phase 2b+ (å¯é€‰)**ï¼šè¿›ä¸€æ­¥ä¼˜åŒ– CLIP å¯¹é½ï¼ˆæ›´å¤§ batchã€SoftCLIP lossã€label smoothingï¼‰
3. **Phase 1b**ï¼šMasking é¢„è®­ç»ƒï¼ˆéœ€ä¸‹è½½ THINGS_MUA_trials.mat ~58GB æˆ–è®¾è®¡ normMUA ä¸Šçš„æ›¿ä»£æ–¹æ¡ˆï¼‰
4. **Phase 3**ï¼šæ¶ˆèå®éªŒï¼ˆé¢„è®­ç»ƒ vs éšæœºã€V1/V4/IT è„‘åŒºï¼‰

---

*æœ¬æ–‡æ¡£å°†éšé¡¹ç›®è¿›å±•æŒç»­æ›´æ–°ã€‚*
